## 为什么需要分布式锁

与分布式锁对应的是单机锁：写多线程程序时，为了避免同时操作进程中的全局变量，通常会使用一把锁来「互斥」，以保证全局变量的正确性。又或者，当我们在同一台机器的不同进程，想要同时操作一个共享资源（例如修改同一个文件），我们可以使用操作系统提供的「文件锁」或「信号量」来做互斥。

高并发业务场景下，部署在不同机器上的业务进程，如果需要同时操作共享资源，为了避免「时序性」问题，通常会借助分布式锁来做互斥，以保证业务的正确性。

可以实现分布式锁的中间件有很多，redis、zookeeper和etcd，高并发业务场景下，为了追求更好的性能，通常选择redis。

## 基于redis的分布式锁如何实现

### 最开始最基础的分布式锁

redis提供了`SETNX (SET if Not eXists)`命令实现互斥，即只有当这个key不存在，才进行设置。

客户端1进行加锁，加锁成功：

```shell
127.0.0.1:6379> SETNX lock 1
(integer) 1     // 客户端1，加锁成功
```

客户端2进行加锁，加锁失败：

```shell
127.0.0.1:6379> SETNX lock 1
(integer) 0     // 客户端2，加锁失败
```

客户端1操作完毕，使用`DEL`释放锁：

```shell
127.0.0.1:6379> DEL lock // 释放锁
(integer) 1
```

### 存在的问题

上述方式存在明显的问题：

1. 客户端1业务逻辑异常，没有及时释放锁
2. 进程挂了，没机会释放锁

针对问题1，在写业务的时候可以提高代码的健壮性，比如go可以用defer关键字确保锁被释放。

针对问题2，我们可以为锁设置过期时间，到期立即释放。

```shell
127.0.0.1:6379> SETNX lock 1    // 加锁
(integer) 1
127.0.0.1:6379> EXPIRE lock 10  // 10s后自动过期
(integer) 1
```

但上述操作依旧存在问题，`EXPIRE`操作可能因为客户端宕机、网络原因或者redis宕机未执行，导致锁不能被释放。

因此我们需要将加锁和设置过期时间原子性执行，Redis 2.6.12 之后，Redis 扩展了`SET`命令的参数，把`NX/EX`集成到了`SET`命令中，用这一条命令就可以了：

```shell
// 一条命令保证原子性执行
127.0.0.1:6379> SET lock 1 EX 10 NX
OK
```

这样是否就没问题了呢？试想这样一种场景：

1. 客户端1加锁成功，开始操作资源
2. 客户端锁过期，但客户端1资源操作还未完成，锁被自动释放
3. 客户端2成功获得锁
4. 客户端1资源操作完成，释放锁，但却释放了客户端2的锁

这里出现了两个问题：

1. 资源未操作完，锁便过期，我们加锁只是为了多个进程操作共享资源的互斥性，如果资源未操作完，锁释放，那就不能保证资源的安全
2. 释放了不属于自己的锁

针对问题1，我们可以先设置1个过期时间，再通过守护线程，定期去检测这个锁的过期时间，如果快过期了，资源操作还没完成，就重新设置过期时间。

针对问题2，我们需要再释放自己锁的时候进行判断当前锁是否是自己的，也就是在加锁的时候加入自己的唯一标识，比如UUID。

```shell
// 锁的VALUE设置为UUID
127.0.0.1:6379> SET lock $uuid EX 20 NX
OK

// 释放锁的伪代码
// 锁是自己的，才释放
if redis.get("lock") == $uuid:
    redis.del("lock")
```

但同样需要保证释放操作是原子性的，不然可能出现以下情况：

1. 客户端1检查到当前锁是自己的
2. 锁刚好过期
3. 客户端2获得锁
4. 客户端1释放了客户端2的锁

由于redis是单线程的，会顺序执行所有操作，因此可以通过lua脚本保证原子性。虽然 Lua 脚本内部可以执行多个命令，但是从 Redis 的角度看，整个脚本的执行就像一个事务一样。这意味着脚本中的所有操作要么全部执行，要么因为错误而全部不执行。并且由于是单线程，在 Lua 脚本执行期间，不会发生任何形式的上下文切换，这避免了并发执行的问题。

```lua
// 判断锁是自己的，才释放
if redis.call("GET",KEYS[1]) == ARGV[1]
then
    return redis.call("DEL",KEYS[1])
else
    return 0
end
```

### 基于redis的分布式锁

1. **加锁**：SET lock unique_id EX $expire_time NX
2. **操作共享资源**：没操作完之前，开启守护线程，定期给锁续期
3. **释放锁**：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁

<img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavwr6Fj34LTeGyicjdNVFiaGKPtae9kEq4Yib7RUqpPf0OJBbBx9XV3LG4R5SqC41bvfMHHATqgRztTg/640?wx_fmt=other&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1">

以上场景是针对单个redis实例，但在实际生产环境中，通常是采用主从集群+哨兵的模式部署。当发生主从切换时，这个分布式锁可能不安全：

1. 客户端1加锁成功
2. 主库异常宕机，同步命令还没同步到从库上
3. 从库被哨兵提升为主库，锁在新的主库上丢失了

## Redlock方案

针对上述问题，redis作者提出了一种名为redlock的解决方案，redlock的前提是：

1. 不部署从库和哨兵实例，只部署主库
2. 主库部署多个，至少5个实例

redlock流程如下：

1. 获取当前时间戳T1
2. 依次向5个主库发送加锁请求，每个请求都会设置1个超时时间（远小于锁的有效时间），如果加锁失败（网络超时、锁被它人持有），就立即向下一个redis实例申请
3. 如果大于等于3个（大多数）个redis实例加锁成功，则再次获取当前时间戳T2，如果T2-T1<锁的过期时间，则认为客户端加锁成功，否则失败。
4. 加锁成功，则操作资源
5. 加锁失败，对每个redis实例尝试释放锁

### Redlock为什么这样做

**1) 为什么要在多个实例上加锁？**

为了容错，即使部分实例宕机，剩余实例加锁成功，整个锁服务依旧可用

**2) 为什么大多数加锁成功，才算成功？**

这是分布式系统的容错问题，这个问题的结论是：**如果只存在「故障」节点，只要大多数节点正常，那么整个系统依旧是可以提供正确服务的。**

> 参考拜占庭将军模型

**3) 为什么步骤 3 加锁成功后，还要计算加锁的累计耗时？**

确保返回加锁成功的所有实例的锁没有过期

**4) 为什么释放锁，要操作所有节点？**

可能有些redis实例已经加上锁了，但由于网络问题，导致我们没收到请求，以为这个实例没加上锁，因此需要对每个实例释放锁，从而确保每个锁被正确释放。

## Redlock的问题

分布式专家 Martin 对于 Redlock 的质疑，在他的文章中，主要阐述了 4 个论点：

### **1) 分布式锁的目的是什么？**

**第一，效率。**

使用分布式锁的互斥能力，是避免不必要地做同样的两次工作（例如一些昂贵的计算任务）。如果锁失效，并不会带来「恶性」的后果，例如发了 2 次邮件等，无伤大雅。

**第二，正确性。**

使用锁用来防止并发进程互相干扰。如果锁失效，会造成多个进程同时操作同一条数据，产生的后果是**数据严重错误、永久性不一致、数据丢失**等恶性问题，就像给患者服用重复剂量的药物一样，后果严重。

他认为，如果你是为了前者——效率，那么使用单机版 Redis 就可以了，即使偶尔发生锁失效（宕机、主从切换），都不会产生严重的后果。而使用 Redlock 太重了，没必要。

**而如果是为了正确性，Martin 认为 Redlock 根本达不到安全性的要求，也依旧存在锁失效的问题！**

### **2) 锁在分布式系统中会遇到的问题**

Martin 表示，一个分布式系统，存在着你想不到的各种异常情况。这些异常场景主要包括三大块，这也是分布式系统会遇到的三座大山：**NPC**。

- N：Network Delay，网络延迟
- P：Process Pause，进程暂停（GC）
- C：Clock Drift，时钟漂移

Martin 用一个进程暂停（GC）的例子，指出了 Redlock 安全性问题：

1. 客户端 1 请求锁定节点 A、B、C、D、E
2. 客户端 1 的拿到锁后，进入 GC（时间比较久）
3. 所有 Redis 节点上的锁都过期了
4. 客户端 2 获取到了 A、B、C、D、E 上的锁
5. 客户端 1 GC 结束，认为成功获取锁
6. 客户端 2 也认为获取到了锁，发生「冲突」

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavwr6Fj34LTeGyicjdNVFiaGK6FXUtAwCRkruxNibic3jJQf2sKG73GQ9yb0BZca3yZ1loxMomo7BXXRA/640?wx_fmt=other&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Martin 认为，GC 可能发生在程序的任意时刻，而且执行时间是不可控的。

> 注：当然，即使是使用没有 GC 的编程语言，在发生网络延迟时，也都有可能导致 Redlock 出现问题，这里 Martin 只是拿 GC 举例。

### **3) 假设时钟正确的是不合理的**

又或者，当多个 Redis 节点「时钟」发生问题时，也会导致 Redlock **锁失效**。

1. 客户端 1 获取节点 A、B、C 上的锁，但由于网络问题，无法访问 D 和 E
2. 节点 C 上的时钟「向前跳跃」，导致锁到期
3. 客户端 2 获取节点 C、D、E 上的锁，由于网络问题，无法访问 A 和 B
4. 客户端 1 和 2 现在都相信它们持有了锁（冲突）

Martin 觉得，Redlock 必须「强依赖」多个节点的时钟是保持同步的，一旦有节点时钟发生错误，那这个算法模型就失效了。

> 即使 C 不是时钟跳跃，而是「崩溃后立即重启」，也会发生类似的问题。

Martin 继续阐述，机器的时钟发生错误，是很有可能发生的：

- 系统管理员「手动修改」了机器时钟
- 机器时钟在同步 NTP 时间时，发生了大的「跳跃」

总之，Martin 认为，Redlock 的算法是建立在「同步模型」基础上的，有大量资料研究表明，同步模型的假设，在分布式系统中是有问题的。在混乱的分布式系统的中，你不能假设系统时钟就是对的，所以，你必须非常小心你的假设。

### **4) 提出 fecing token 的方案，保证正确性**

相对应的，Martin 提出一种被叫作 fecing token 的方案，保证分布式锁的正确性。这个模型流程如下：

1. 客户端在获取锁时，锁服务可以提供一个「递增」的 token
2. 客户端拿着这个 token 去操作共享资源
3. 共享资源可以根据 token 拒绝「后来者」的请求

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavwr6Fj34LTeGyicjdNVFiaGKNbLib8umCTlrBexwlhhQw8WkoS3sVWdx9UEed1pmicqrj2PZ19j7jxTA/640?wx_fmt=other&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这样一来，无论 NPC 哪种异常情况发生，都可以保证分布式锁的安全性，因为它是建立在「异步模型」上的。而 Redlock 无法提供类似 fecing token 的方案，所以它无法保证安全性。他还表示，**一个好的分布式锁，无论 NPC 怎么发生，可以不在规定时间内给出结果，但并不会给出一个错误的结果。也就是只会影响到锁的「性能」（或称之为活性），而不会影响它的「正确性」。**

Martin 的结论：

**1、Redlock 不伦不类**：它对于效率来讲，Redlock 比较重，没必要这么做，而对于正确性来说，Redlock 是不够安全的。

**2、时钟假设不合理**：该算法对系统时钟做出了危险的假设（假设多个节点机器时钟都是一致的），如果不满足这些假设，锁就会失效。

**3、无法保证正确性**：Redlock 不能提供类似 fencing token 的方案，所以解决不了正确性的问题。为了正确性，请使用有「共识系统」的软件，例如 Zookeeper。

## Redlock对于问题的解释

### **1) 解释时钟问题**

Redis 作者表示，Redlock 并不需要完全一致的时钟，只需要大体一致就可以了，允许有「误差」。例如要计时 5s，但实际可能记了 4.5s，之后又记了 5.5s，有一定误差，但只要不超过「误差范围」锁失效时间即可，这种对于时钟的精度的要求并不是很高，而且这也符合现实环境。

对于对方提到的「时钟修改」问题，Redis 作者反驳到：

1. **手动修改时钟**：不要这么做就好了，否则你直接修改 Raft 日志，那 Raft 也会无法工作...
2. **时钟跳跃**：通过「恰当的运维」，保证机器时钟不会大幅度跳跃（每次通过微小的调整来完成），实际上这是可以做到的

### **2) 解释网络延迟、GC 问题**

之后，Redis 作者对于对方提出的，网络延迟 wan、进程 GC 可能导致 Redlock 失效的问题，也做了反驳：

在Redlock步骤 3，加锁成功后为什么要重新获取「当前时间戳 T2」？还用 T2 - T1 的时间，与锁的过期时间做比较？

Redis 作者强调：如果在 1-3 发生了网络延迟、进程 GC 等耗时长的异常情况，那在第 3 步 T2 - T1，是可以检测出来的，如果超出了锁设置的过期时间，那这时就认为加锁会失败，之后释放所有节点的锁就好了！

Redis 作者继续论述，如果对方认为，发生网络延迟、进程 GC 是在步骤 3 之后，也就是客户端确认拿到了锁，去操作共享资源的途中发生了问题，导致锁失效，那这**不止是 Redlock 的问题，任何其它锁服务例如 Zookeeper，都有类似的问题，这不在讨论范畴内**。

这里我举个例子解释一下这个问题：

1. 客户端通过 Redlock 成功获取到锁（通过了大多数节点加锁成功、加锁耗时检查逻辑）
2. 客户端开始操作共享资源，此时发生网络延迟、进程 GC 等耗时很长的情况
3. 此时，锁过期自动释放
4. 客户端开始操作 MySQL（此时的锁可能会被别人拿到，锁失效）

Redis 作者这里的结论就是：

- 客户端在拿到锁之前，无论经历什么耗时长问题，Redlock 都能够在第 3 步检测出来
- 客户端在拿到锁之后，发生 NPC，那 Redlock、Zookeeper 都无能为力

所以，Redis 作者认为 Redlock 在保证时钟正确的基础上，是可以保证正确性的。

### **3) 质疑 fencing token 机制**

Redis 作者对于对方提出的 fecing token 机制，也提出了质疑，主要分为 2 个问题：

**第一**，这个方案必须要求要操作的「共享资源服务器」有拒绝「旧 token」的能力。

例如，要操作 MySQL，从锁服务拿到一个递增数字的 token，然后客户端要带着这个 token 去改 MySQL 的某一行，这就需要利用 MySQL 的「事务隔离性」来做。

```sql
// 两个客户端必须利用事务和隔离性达到目的
// 注意 token 的判断条件
UPDATE
    table T
SET
    val = $new_val, current_token = $token
WHERE
    id = $id AND current_token < $token
```

但如果操作的不是 MySQL 呢？例如向磁盘上写一个文件，或发起一个 HTTP 请求，那这个方案就无能为力了，这对要操作的资源服务器，提出了更高的要求。也就是说，大部分要操作的资源服务器，都是没有这种互斥能力的。**再者，既然资源服务器都有了「互斥」能力，那还要分布式锁干什么？**

**第二**，退一步讲，即使 Redlock 没有提供 fecing token 的能力，但 Redlock 已经提供了随机值（就是前面讲的 UUID），利用这个随机值，也可以达到与 fecing token 同样的效果。

1. 客户端使用 Redlock 拿到锁
2. 客户端在操作共享资源之前，先把这个锁的 VALUE，在要操作的共享资源上做标记
3. 客户端处理业务逻辑，最后，在修改共享资源时，判断这个标记是否与之前一样，一样才修改（类似 CAS 的思路）

还是以 MySQL 为例，举个例子就是这样的：

1. 客户端使用 Redlock 拿到锁
2. 客户端要修改 MySQL 表中的某一行数据之前，先把锁的 VALUE 更新到这一行的某个字段中（这里假设为 current_token 字段)
3. 客户端处理业务逻辑
4. 客户端修改 MySQL 的这一行数据，把 VALUE 当做 WHERE 条件，再修改

```sql
UPDATE
    table T
SET
    val = $new_val
WHERE
    id = $id AND current_token = $redlock_value
```

可见，这种方案依赖 MySQL 的事务机制，也达到对方提到的 fecing token 一样的效果。

但这里还有个小问题，是网友参与问题讨论时提出的：**两个客户端通过这种方案，先「标记」再「检查+修改」共享资源，那这两个客户端的操作顺序无法保证啊？**而用 Martin 提到的 fecing token，因为这个 token 是单调递增的数字，资源服务器可以拒绝小的 token 请求，保证了操作的「顺序性」！Redis 作者对于这个问题做了不同的解释，我觉得很有道理，他解释道：**分布式锁的本质，是为了「互斥」，只要能保证两个客户端在并发时，一个成功，一个失败就好了，不需要关心「顺序性」。**

综上，Redis 作者的结论：

**1、作者同意对方关于「时钟跳跃」对 Redlock 的影响，但认为时钟跳跃是可以避免的，取决于基础设施和运维。**

**2、Redlock 在设计时，充分考虑了 NPC 问题，在 Redlock 步骤 3 之前出现 NPC，可以保证锁的正确性，但在步骤 3 之后发生 NPC，不止是 Redlock 有问题，其它分布式锁服务同样也有问题，所以不在讨论范畴内。**

## 基于zookeeper的分布式锁

zookeeper实现的分布式锁如下：

1. 客户端1和2都尝试创建临时节点，比如/lock
2. 客户端1先到达，则加锁成功
3. 客户端1操作共享资源
4. 客户端1删除/lock节点，释放锁

通过采用临时节点，只要客户端连接不断，就会一直持有锁，不会过期。并且如果客户端崩溃宕机了，这个临时节点也会自动删除，保证锁被释放。

由于客户端需要保持连接，才能持有锁。因此客户端会和zookeeper服务器维护一个session，这个session依赖客户端“定时心跳”来维持连接，如果 Zookeeper 长时间收不到客户端的心跳，就认为这个 Session 过期了，也会把这个临时节点删除。

同样地，基于此问题，我们也讨论一下 GC 问题对 Zookeeper 的锁有何影响：

1. 客户端 1 创建临时节点 /lock 成功，拿到了锁
2. 客户端 1 发生长时间 GC
3. 客户端 1 无法给 Zookeeper 发送心跳，Zookeeper 把临时节点「删除」
4. 客户端 2 创建临时节点 /lock 成功，拿到了锁
5. 客户端 1 GC 结束，它仍然认为自己持有锁（冲突）

可见，即使是使用 Zookeeper，也无法保证进程 GC、网络延迟异常场景下的安全性。

## 基于Etcd的分布式锁

基于Etcd的分布式锁如下：

1. 客户端1创建1个租约（设置过期时间）
2. 客户端1携带这个租约，创建/lock节点
3. 客户端1发现节点不存在，获得锁
4. 客户端2尝试创建节点，获取锁失败
5. 客户端1定时给租约续期，保持自己一直持有锁
6. 客户端1操作共享资源
7. 客户但1删除/lock节点，释放锁

定时给租约续期的步骤，和上面 Zookeeper 客户端定时给 Server 发心跳类似，其目的都是让服务端保持这个 Session 或 KV 持续有效。

所以，它依旧存在和 Zookeeper 相同的问题：

1. 客户端 1 创建节点 /lock 成功，拿到了锁
2. 客户端 1 发生长时间 GC
3. 客户端 1 无法向 Etcd 发请求给租约「续期」
4. 租约到期，Etcd 「删除」锁节点
5. 客户端 2 创建临时节点 /lock 成功，拿到了锁
6. 客户端 1 GC 结束，它仍然认为自己持有锁（冲突）

可见，基于 Etcd 实现的分布锁，当拿到锁发生 GC、网络延迟问题，依旧可能失效。

至此，这里我们可以得出结论：**一个分布式锁，无论是基于 Redis 还是 Zookeeper、Etcd 实现，在极端情况下，都无法保证 100% 安全，都存在失效的可能。**如果你的业务数据非常敏感，在使用分布式锁时，一定要注意这个问题，不能假设分布式锁 100% 安全。

## 到底要不要用Redlock

**1) 到底要不要用 Redlock？**

前面也分析了，Redlock 只有建立在「时钟正确」的前提下，才能正常工作，如果你可以保证这个前提，那么可以拿来使用。

但保证时钟正确，并不是你想的那么简单就能做到的。

**第一，从硬件角度来说**，时钟发生偏移是时有发生，无法避免的。例如，CPU 温度、机器负载、芯片材料都是有可能导致时钟发生偏移。

**第二**，人为错误也是很难完全避免的。

对于 Redlock，尽量不用它，而且它的性能不如单机版 Redis，部署成本也高，我还是会优先考虑使用 Redis「主从+哨兵」的模式，实现分布式锁。

那正确性如何保证呢？第二点给你答案。

**2) 如何正确使用分布式锁？**

在分析 Martin 观点时，它提到了 fecing token 的方案，给我了很大的启发，虽然这种方案有很大的局限性，但对于保证「正确性」的场景，是一个非常好的思路。

所以，我们可以把这两者结合起来用：

**1、使用分布式锁，在上层完成「互斥」目的，虽然极端情况下锁会失效，但它可以最大程度把并发请求阻挡在最上层，减轻操作资源层的压力。**

**2、但对于要求数据绝对正确的业务，在资源层一定要做好「兜底」，设计思路可以借鉴 fecing token 的方案来做，即在 DB 层通过版本号的方式来更新数据，避免并发冲突。**