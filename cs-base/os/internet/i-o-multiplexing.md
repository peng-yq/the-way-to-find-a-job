## 最基本的socket模型

通过socket可以在客户端和服务器中通信，实现进程间的跨主机通信。双方进行通信前需要各自创建一个socket，用于读取和发送数据。创建 socket的时候，可以指定网络层使用的是IPv4还是IPv6，传输层使用的是TCP还是UDP。相较于TCP，UDP的socket编程更加简单。

服务端和客户端的socket编程：

- 首先通过socket()创建一个基于IPV4/TCP的socket，调用bind()给这个socket绑定IP:Port。
  - 绑定端口的目的：内核收到tcp报文后，会检查里面的端口号，通过这个端口号把数据转发到对应的应用程序
  - 绑定IP地址的目的：一台计算机可能有多个网卡，每个网卡的都有对应的IP地址，只有绑定了网卡，内核才会把对应网卡的数据转发至应用程序
- 绑定成功后，服务端通过listen()进行监听
- 进入监听状态后，调用accept()函数等待客户端的连接，如果没有客户端的连接则会阻塞
- 而客户端在创建好socket后，则通过connet()发起连接，该函数需要指明服务端的IP:Port，然后就是TCP的三次握手
- 在三次握手的过程中，服务器的内核为每个socket维护了两个队列：TCP半连接队列（没有完成三次握手，服务端处于syn_rcvd）和TCP全连接队列（完成三次握手，服务端处于established）
- 当全连接队列不为空时，accept()从其中取出一个已完成连接的socket返回应用程序，后续和客户端的数据传输都使用这个socket
- 建立连接后，双方可通过read()和write()来读写数据（或recv()和send()）
- 通信结束后，双方应通过close()函数关闭socket，以释放系统资源

从上述流程中可以看出，客户端和服务端的通信需要2个socket：

1. 用于监听的socket
2. 已连接的socket（用于读写数据）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/tcp_socket.png">

得益于“Linux一切皆文件”的理念，我们对socket进行读写，本质是在对文件进行读写，每个socket都有对应的文件描述符。

### task_struct，文件描述符和socket_struct

在Linux操作系统中，task_struct是一个非常重要的结构体，用于描述一个进程的状态。这个结构体包含了操作系统需要的几乎所有信息来管理进程，每个进程在内核中都有一个对应的task_struct实例。task_struct结构体的字段非常多，以下是一些主要的字段：

- state: 表示进程的状态（例如，运行、可中断睡眠、不可中断睡眠、停止、僵尸等）。
- pid: 进程的唯一标识符。
- priority: 进程的动态优先级。
- static_prio: 进程的静态优先级。
- normal_prio: 进程的普通优先级。
- parent: 指向父进程的指针。
- children: 进程的子进程列表。
- sibling: 同一个父进程的其他子进程链表。
- mm: 指向内存描述符mm_struct的指针，包含进程的所有内存信息。
- files: 指向打开文件描述符的结构体。
- fs: 用于文件系统导航的信息。
- signal: 信号相关的信息。
- sighand: 信号处理的结构体。
- thread: 线程特定的信息，如寄存器状态等。
- task_list: 用于将进程链接到全局进程列表的链表节点。

task_struct中有一个指向「文件描述符数组」的成员指针，该数组里列出这个进程打开的所有文件的文件描述符。数组的下标是文件描述符，是一个整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，内核可以通过文件描述符找到对应打开的文件。

> 每个进程都有自己独立的一个从零开始的文件描述符空间，所以如果运行了两个不同的程序，对应两个不同的进程，如果它们都打开一个文件，它们或许可以得到相同数字的文件描述符，但是因为内核为每个进程都维护了一个独立的文件描述符空间，这里相同数字的文件描述符可能会对应到不同的文件。

Linux中每个文件/目录都有一个inode（inode包含了关于文件的大部分信息，但不包括文件名和文件数据），Socket文件的inode指向了内核中的Socket结构，**在这个结构体里有两个队列，分别是发送队列和接收队列，这个两个队列里面保存的是一个个struct sk_buff**，用链表的组织形式串起来。sk_buff =可以表示各个层的数据包，在应用层数据包叫data，在TCP层我们称为segment，在IP层我们叫 packet，在数据链路层称为frame。

为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，**如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低CPU效率**。于是，为了在层级之间传递数据时，不发生拷贝，通过调整sk_buff中data的指针（指向当前正在处理的数据的起始位置），从而实现一个结构体来描述所有的网络包。比如：

- 当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加skb->data的值，来逐步剥离协议首部。
- 当要发送报文时，创建sk_buff结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少skb->data的值来增加协议首部。

> 当然不只是通过data，其他的指针诸如len和tail也是在变化的，这里的增加和减少是相对于顶部而言

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/sk_buff.jpg">

但上述模型采用同步阻塞的方式使得同一时间只能服务一个用户。此外根据TCP连接的四元组：本机IP, 本机端口, 对端IP, 对端端口，理论上服务端单机最大TCP连接数约为2^48 (2^32*2^16，目标IP和目标端口号进行组合)，但现实受限于以下两个条件

- 单个进程打开的文件描述符是有限的，默认是1024
- 每个连接需要消耗一定的系统内存和资源

## 多进程模型

相较于上述同步阻塞型socket模型，如果服务器要同时支持多个客户端，最传统的方法则是多进程模型，为每个客户端分配一个进程来处理请求。

多进程模型中，主进程通过accept()监听客户端的连接，一旦连接完成，accpet()将会返回一个已连接socket。主进程通过fork创建一个子进程来处理对该客户端的请求。fork会把父进程的东西都复制一遍，例如文件描述符、代码和地址内存空间等。因此子进程此时可以直接与客户端通信并处理请求。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E5%A4%9A%E8%BF%9B%E7%A8%8B.png">

子进程退出时，主进程需要及时对其资源进行回收（可以通过wait和waitpid），否则将会成为僵尸进程，从而消耗系统资源。多进程模型只能应对少量的客户端连接，因为进程上下文的切换是需要代价的。

## 多线程模型

相较于多进程模型，我们可以在单进程中可以运行多个线程，同进程里的线程可以共享进程的部分资源，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。

服务器通过TCP和客户端连接后，通过pthread_create()函数创建线程，然后将已连接Socket的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。为了避免线程的频繁创建和销毁，可以提前创建若干个线程也就是线程池，当与客户端建立连接后，将已连接socket放入队列，线程池中的线程从队列中取出socket进行处理。当然由于这个队列是全局的，还需要进行并发控制，比如信号量和锁等内容。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E7%BA%BF%E7%A8%8B%E6%B1%A0.png">

不管是多进程模型还是多线程模型，每建立一个TCP连接就需要一个进程或线程进行处理，对于C10K问题，则需要维护1万个进程或线程，这是不可接受的。

## I/O多路复用

解决上述问题的办法则是通过I/O多路复用，也就是**使用一个进程来维护多个socket**。CPU虽然一次只能处理一个请求，但处理一次请求的时间很短，1s内可以处理上千个请求，把时间拉长来看相当于多个请求复用了一个进程，和os的进程并发类似，也叫时分多路复用。内核提供的I/O多路复用包括**select/poll和epoll，可以通过一个系统调用从内核中获取多个事件，通过将所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，在用户态进行处理**。

### select/poll

select实现多路复用的过程可以简单阐述如下：**select将所有已连接的socket放入一个文件描述符集合，再通过select将文件描述符集合拷贝至内核，内核对这个集合的所有文件描述符进行遍历，发现有事件产生则将对应的socket标记为可读或可写。再将文件描述符集合拷贝至用户态，用户态对这个集合进行再次遍历找到可读或可写的socket进行处理**。select的过程中涉及2次对文件描述符集合的遍历和拷贝。此外，**select使用固定长度的BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的**，在Linux系统中，由内核中的FD_SETSIZE限制， 默认最大值为1024，只能监听0~1023的文件描述符。

相较于select，poll使用动态数组，以链表的形式来组织，突破了位图固定长度的限制，只受限于系统资源和内存（但受限于系统描述符总数的限制）。poll和select并没有太大的本质区别，**都是使用线性结构存储进程关注的socket集合（只不过一个数量有限，一个“无限”），都需要遍历文件描述符集合来找到可读或可写的socket，时间复杂度为O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### epoll

epoll是对poll的进一步改进，特别是在处理大量文件描述符时。epoll使用一种更为高效的机制，它只关注活跃的文件描述符，而不是遍历整个文件描述符集：

- epoll使用事件通知方式，只返回活动的文件描述符，减少了不必要的检查和内存拷贝。
- epoll适合处理大规模文件描述符，因为它的性能几乎不受监视的文件描述符数量的影响。

```c
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...);
    for(接收到数据的socket){
        //处理
    }
}
```

epoll的使用方法如上所示，首先通过epoll_create创建一个epfd对象，将所有需要监视的socket放入这个epfd对象中，最后调用epoll_wait等待事件通知再进行处理。

epoll是怎样解决select/poll的问题呢？

1. epoll在内核里使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的socket通过epoll_ctl()函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是O(logn)。而select/poll内核里没有类似epoll红黑树这种保存所有待检测的socket的数据结构，所以select/poll每次操作时都传入整个socket集合给内核，而epoll因为在内核维护了红黑树，可以保存所有待检测的socket，所以只需要传入一个待检测的socket，减少了内核和用户空间大量的数据拷贝和内存分配。
2. epoll使用事件驱动的机制，内核里维护了一个链表来记录就绪事件，当某个socket有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用epoll_wait()函数时，只会返回有事件发生的文件描述符的个数，不需要像select/poll那样轮询扫描整个socket集合，大大提高了检测的效率。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png">

epoll的方式即使监听的Socket数量越多的时候，效率不会大幅度降低，能够同时监听的Socket的数目上限就为系统定义的进程打开的最大文件描述符个数。

#### 边缘触发和水平触发

epoll支持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）。

- 边缘触发：当被监控的socket有事件发生时，例如可读或可写，服务端只会从epoll_wait中苏醒一次。因此需要保证一次性将内核缓冲区的数据读取完毕。
- 水平触发：被监控的socket有事件发生时，服务端会不断的从epoll_wait苏醒，直到把内核缓冲区的数据读取完毕。

以取快递为例，边缘触发相当于你有一个快递到了，只给你发一次消息，即使你一直没取；而水平触发则是只要你没取就一直给你发消息。

如果使用边缘触发模式，I/O事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。**因此，我们会循环从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，边缘触发模式一般和非阻塞I/O搭配使用，程序会一直执行I/O操作，直到系统调用（如 read 和 write）返回错误，错误类型为EAGAIN或EWOULDBLOCK。一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少epoll_wait的系统调用次数。select/poll只有水平触发模式，epoll默认的触发模式是水平触发**，但是可以根据应用场景设置为边缘触发模式。另外，使用I/O多路复用时，最好搭配非阻塞I/O一起使用（返回的事件可能是不可读写的，比如发生错误被丢弃）。

