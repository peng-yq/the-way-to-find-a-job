由于磁盘读写特别慢，与内存速度相差可达10倍以上，有很多针对磁盘优化的方法用于提高系统吞吐量：

- 零拷贝
- 直接I/O（对应缓存I/O，也就是是否需要将磁盘中的数据拷贝至内存缓冲区的问题）
- 异步I/O
- Linux内核页高速缓存（PageCache）

## DMA

没有DMA之前的I/O（以读磁盘中的数据举例）：

- 进程发起read请求：
    - 进程通过调用read系统调用发起读取操作。这时，控制权从用户态转移到内核态（这种转换通常被称为“系统调用”或“陷入内核”）。
- 内核处理I/O请求：
    - 内核接收到读取请求后，解析请求并确定需要从磁盘读取的数据位置。
    - 内核向磁盘控制器发送I/O请求。这通常涉及向设备驱动程序发送命令，设备驱动程序再将这些命令转化为对具体硬件设备的操作。
- 磁盘控制器和数据传输：
    - 磁盘控制器处理来自内核的命令，从磁盘中读取数据到其内部缓冲区（通常称为磁盘缓冲区或硬件缓冲区）。
    - 一旦数据被读取到磁盘控制器的缓冲区，磁盘控制器会向CPU发出一个中断信号，表明数据已准备好被处理（准确来说应该是磁盘缓冲区慢了后发出中断信号）。
- OS处理中断：
    - 操作系统响应中断，保存当前进程的上下文（如CPU寄存器等状态），并处理中断。
    - 内核将数据从磁盘控制器的缓冲区复制到操作系统的页缓存（PageCache）中。
- 数据传输到用户空间：
    - 内核再将数据从页缓存复制到用户提供的缓冲区，也就是从内核空间到用户空间。
    - 一旦数据被复制到用户缓冲区，read系统调用完成，结果返回给用户程序。
- 返回用户态：
    - 系统调用完成后，操作系统将控制权返回给原始调用者，从内核态切换回用户态。

> 在Linux操作系统中，PageCache（页高速缓存）是内核用来缓存从硬盘读取的文件数据的一部分内存（局部性）。这样做的目的是为了提高文件访问的速度。当文件数据被读取时，它首先被存储在PageCache中。如果之后再次需要这些数据，操作系统可以直接从内存中获取，而不是重新从较慢的硬盘读取。这显著减少了访问延迟和提高了性能。如果pagecache中的空间不足了，会使用LRU淘汰最久未访问的缓存。另外，OS还可以将多个小的I/O请求合并成一个大的I/O请求。此外，PageCache还提供了预读功能。比如，假设read方法每次只会读32 KB的字节，虽然read刚开始只会读0 ～ 32 KB的字节，但内核会把其后面的32～64 KB也读取到PageCache，这样后面读取32～64 KB的成本就很低（比起读磁盘而言），如果在32～64 KB淘汰出PageCache前，进程读取到它了，收益就非常大。总体来说，虽然pagecache多了一次复制，但可以提高系统的安全性、稳定性和效率。

> 局部性原理基于这样一个观察：计算机程序倾向于重复使用近期使用过的数据项或者接近最近使用过的数据项的数据，也就是时间局部性（Temporal Locality）和空间局部性（Spatial Locality）。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png">

上述过程中，涉及到数据的多次转移：磁盘->磁盘控制器缓冲区->pagecache->用户缓冲区，需要CPU对数据进行“搬运”，在传输大量数据（千兆网卡或从磁盘读大量数据）时，数据传输效率会很低并且CPU负荷会特别大。于是有了**DMA（Direct Memory Access）**技术。

DMA技术出现后，上述I/O过程中由CPU搬运数据到pagecache的步骤由DMA控制器执行。当使用DMA时，CPU会初始化传输，之后它可以执行其他任务，直到数据传输完成时接收一个中断信号。这样，DMA可以在不占用CPU资源的情况下进行大量数据的快速传输。**需要注意的是，DMA只负责硬件系统数据到pagecache的传输；pagecache到进程缓冲区的数据传输仍然需要CPU**。早期DMA只存在于主板中，随着I/O设备的增多和需求不同，每个I/O设备都有自己的DMA。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png">

## 传统的文件传输（C/S架构）

当客户端向服务端请求数据/文件时，通常需要服务端将数据/文件从磁盘中读取出来，再通过网络协议发送至客户端。上述过程通常需要用到下列系统调用：

```c
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png">

可以看到上述过程中发生了4次用户态/内核态的转换和4次数据拷贝，而我们只需要一次数据传输，这里面os状态切换和上下文保存以及数据拷贝都需要一定的时间和资源消耗，在高并发的场景中，性能和时延被累计和放大。因此，**提升系统性能需要减少用户态和内核态的上下文的切换和内存拷贝的次数**。

## 零拷贝

> 零拷贝是对于CPU而言，也就是没有从内存层面区拷贝数据，数据的拷贝由DMA直接完成。

对于减少用户态和内核态的上下文切换关键在于减少系统调用的次数，一次系统调用必然会产生两次上下文切换。而单纯文件的传输过程中，无需对数据进行加工，用户的缓冲区就显得没有必要了。**注意零拷贝技术只适合对用户进程数据不用加工的场景**。

零拷贝技术通常有两种：
- mmap + write
- sendfile

### mmap + write

通过使用mmap可以替换read，从而把“将磁盘中的数据/文件读取至用户缓冲区”转换为“将磁盘中的数据/文件读取至os的缓冲区即pagecache，再将这部分数据映射至用户空间，从而减少内核缓冲区到用户缓冲区的拷贝”。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png">

通过mmap可以将内核缓冲区的数据映射至用户空间中，从而减少1次数据拷贝（记住不是两次噢，拷贝到socket缓冲区的步骤仍然是不能少的）。但是这样不就会产生安全问题了吗？内核又是如何避免的呢？

- 权限控制，内核会检查调用进程是否有足够的权限来访问对应的内存区域
- 页保护机制，通过页保护位（rwx），还可以使用cow来防止用户对共享内存的修改来影响其他进程和系统稳定性
- 地址空间布局随机化（ASLR）
- ...

mmap+write还不是最理想的零拷贝，因为只减少了一次数据拷贝，系统调用仍然是2次。

### sendfile

在Linux内核版本2.1中（现在稳定版已经到6.7.12了），提供了一个专门发送文件的系统调用函数sendfile()，函数形式如下：

```c++
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。通过sendfile，可以将系统调用减少为1次，减少两次内核态/用户态上下文切换。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png">

但上述过程还不是真正的零拷贝技术，因为还有一次CPU的拷贝即将数据从内核缓冲区搬运到socket缓冲区。如果设备网卡支持SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过CPU把内核缓冲区里的数据拷贝到socket缓冲区的过程。也就是只需1次系统调用，2次用户态/内核态上下文切换和2次数据拷贝（**注意这里还是需要先将磁盘中的数据拷贝至内存缓冲区**），极大的提高了数据/文件传输的性能。

```shell
# pyq @ 0x505951-Y7000P in ~ [15:07:42]
$ ethtool -k eth0 | grep scatter-gather
scatter-gather: on
        tx-scatter-gather: on
        tx-scatter-gather-fraglist: off [fixed]

# pyq @ 0x505951-Y7000P in ~ [15:07:43]
$ uname -a
Linux 0x505951-Y7000P 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

### 使用零拷贝技术的项目

Kafka：是一个分布式流处理平台，主要用于构建实时的数据流应用和高性能的数据管道。可以处理来自多个来源的大量数据，并能够以高吞吐量和低延迟的方式向多个消费者提供数据。Kafka在许多企业中广泛使用，包括在金融服务、电信、零售、物联网 (IoT)、物流和许多其他领域。

Nginx

## 大文件的传输

虽然通过pagecache的缓存和预读功能，可以提升系统性能。但系统传输大文件/大数据时，pagecache的优势将会被削弱或消失，因为pagecache的容量是很小的，内容将会被迅速替代。另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来2个问题：

- pageCache由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到pageCache，于是这样磁盘读写的性能就会下降了
- pageCache中的大文件数据，由于没有享受到缓存带来的好处，但却耗费DMA多拷贝到pageCache一次

既然这样，针对大文件的传输，我们需要避免将数据拷贝至pagecache，直接拷贝至用户缓冲区。此外，用户进程在发起I/O调用后就会进入阻塞状态，直到内核返回，这个问题可以通过异步I/O来解决。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E5%BC%82%E6%AD%A5%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png">

异步I/O并没有涉及到PageCache，所以使用异步I/O就意味着要绕开PageCache。**绕开PageCache的I/O叫直接I/O，使用PageCache的I/O则叫缓存I/O。通常，对于磁盘，异步I/O只支持直接I/O**。在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。

直接I/O的场景：

1. 大文件的传输
2. 应用程序已经实现了数据的缓存，不需要OS再通过pagecache来再次缓存了

而对于小文件，则更适合pagecache并使用零拷贝。在nginx中可以根据不同文件的大小设置传输方式。

